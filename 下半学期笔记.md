$$\mathcal{Liu~Yuanyi}
\newcommand{\char}{\operatorname{char}}
\newcommand{\ncm}{\newcommand}
\ncm{\fr}[2]{\dfrac{#1}{#2}}
\ncm{\rk}{\operatorname{rk}}
\ncm{\F}{\forall}
\ncm{\d}{\mathrm{d}}
\ncm{\dx}{\d x}
\ncm{\dy}{\d y}
\ncm{\dz}{\d z}
\ncm{\du}{\d u}
\ncm{\dv}{\d v}
\ncm{\dw}{\d w}
\ncm{\dt}{\d t}
\ncm{\ds}{\d s}
\ncm{\dis}{\displaystyle}
\ncm{\E}{\exists}
\ncm{\exist}{\exists}
\ncm{\F}{\forall}
\ncm{\id}{\operatorname{id}}
\ncm{\many}[2][n]{#2_{1},#2_{2},\cdots,#2_{#1}}
\newcommand{\eval}[1]{\l[\!\l[#1\r]\!\r]}

\renewcommand{\le}{\leqslant}
\renewcommand{\leq}{\leqslant}
\renewcommand{\ge}{\geqslant}
\renewcommand{\geq}{\geqslant}%把不等号的一杠抬上去
\renewcommand{\i}{\mathrm{i}}%虚数单位
\newcommand{\sgn}{\mathrm{sgn}}%符号函数
\newcommand{\A}{\forall}%排列数
\renewcommand{\C}{\mathbf{C}}%组合数
\newcommand{\da}{\displaystyle}%连加连乘积分号变大
\newcommand{\mN}{\mathbb{N}}%自然数集
\newcommand{\mR}{\mathbb{R}}%实数集
\newcommand{\mZ}{\mathbb{Z}}%整数集
\newcommand{\N}{\mathbb{N}}%自然数集
\newcommand{\R}{\mathbb{R}}%实数集
\newcommand{\Z}{\mathbb{Z}}%整数集
\newcommand{\mC}{\mathbb{C}}%复数集
\newcommand{\Abs}{\mathrm{Abs}}%绝对值函数
\newcommand{\len}{\operatorname{len}}%绝对值函数

\ncm{\flr}[1]{\left\lfloor #1 \right\rfloor}
\ncm{\com}{\binom}
$$

# Counting 计数

- ~~加法原理和乘法原理~~ (逆天)

- 阶乘$n! =n(n-1)\cdots1$
- 排列数$\mathrm{A}_{n}^{k}=n(n-1)\cdots(n-k+1)= \dfrac{n!}{(n-k)!}=n^{\underline{k}}$
- 组合数$\mathrm{C}_{n}^{k}=\left(\begin{array}{c}  n\\ k\end{array}\right)= \dfrac{n!}{(n-k)!k!}= \dfrac{n^{\underline{k}}}{k!}$

### 组合数的性质
- $\com nk=\com n{n-k}$
- 二项式定理
- $\com nk=\com{n-1}k+\com{n-1}{k-1}$
- $\com nk= \dfrac{n}{k}\com{n-1}{k-1}$
- $\F \left|x\right|<1, r\in \mathbb{R},(1+x)^{r}=\dis\sum_{i=0}^{\infty}\com{r}{i}x^{i}$
	- 推论:  $\dis\sum_{i=0}^{n}\com{n}{i}=2^{n}$

### 组合恒等式
E.g.
$$\begin{align}
\sum_{k=0}^{n} k\com{n}{k}=\sum_{k=1}^{n} n\com{n-1}{k-1}=n2^{n-1}
\end{align}$$
$$\small\sum_{k=0}^{n} \com{n+k}{n}=\com{n+1}{n+1}+\com{n+1}{n}+\com{n+2}{n}+\cdots+\com{n+m}n\xlongequal{重复合并最前两项}\com{n+m}{n+1}$$

*这勾起了当年使用各种奇技淫巧的古早回忆, 但如今已经尽数忘却了.....随意上网搜了一下比如[这个链接](https://icychlorine.github.io/2024/07/27/%E7%BB%84%E5%90%88%E6%95%B0%E5%AD%A6%E4%B8%AD%E7%9A%84%E6%81%92%E7%AD%89%E5%BC%8F/)记载了一些基本的方法和简单的例题*

### 卡特兰数

等价定义:

1) 在平面中从$(0,0)$走到$(n,n)$且始终位于$y=x$下方(含)的路径数
2) 圆上有 $2n$ 个两两不同的点，将这些点成对连接起来且使得所得到的 $n$ 条线段两两不交的方案数
3) 使用对角线把凸 $n+2$ 边形划分成三角形的方法数
4) 由 $n$ 对括号构成的合法的括号序列数
5) 给式子 $0+1+\cdots+n$ 不断添加括号直至明确运算顺序的方法数
6) 有 $n$ 个节点的不同形态的二叉树数(等价地, 含有 $n+1$ 个叶子节点的满二叉树)

还有其他定义, 但都显然与上述某个等价 (尤其是括号序列) , 故不赘述

课上采用定义 1 ,  现计算其通项:

##### 通项:

考察所有可行的路线共$\com{2n}{n}$条, 我们希望计算所有不合法的路径
对每一条不合法的路径, 它一定与$y=x+1$有交点, 将最左下的交点的左下方的部分关于$y=x+1$翻转, 则构成了不合法的路径与从$(-1,1)$到$(n,n)$的路径的一一映射, 也即$\com{2n}{n-1}$

因此卡特兰数$$C_{n}=\com{2n}{n}-\com{2n}{n-1}=\dfrac{1}{n+1}\com{2n}{n}$$
##### 递推:

$$C_{n+1}=\sum_{i=0}^{n} C_{i}C_{n-i}$$

此公式可由很多个定义得出, (进而结合初始值也能证明这些定义等价), 参见[OI-wiki](https://oi-wiki.org/math/combinatorics/catalan/)

### Balls & Bins 问题

把$m$个球装在$n$个桶内, 

1) 球之间有区别/无区别
2) 桶之间有区别/无区别
3) 允许空桶/禁止空桶

那么有多少种装法? (一共是八个问题)

1. 球可区分, 桶可区分


### 容斥原理

$$\left|\bigcup_{i=1}^{n}A_{i}\right|=\sum_{m=1}^{n} (-1)^{m-1}\sum_{1\leqslant a_{1}<a_{2}<\cdots<a_{m}\leqslant n}\left|\bigcap_{i=1}^{m}A_i\right|$$

##### *推广(非课上内容)*

记$S_{k}$为至少属于$k$个$A_{i}$的元素构成的集合, 原版的同斥原理事实上在求$S_{1}$

$$S_{k}=\sum_{i=k}^n(-1)^{i-k}\binom{i}{k}\alpha(i)$$

其中 $$\alpha(k)=\sum_{T\subseteq\{1,2,...,n\},|T|=k}\left |  \bigcap_{i\in T}A_i \right |$$
### 鸽笼原理

>例题
>
> $S\subseteq \left\{1,2,\cdots,2n\right\},\left|S\right|\geqslant n+1$
> 1. $\E i,j\in S,i\neq j,\quad s.t.\;\gcd(i,j)=1$
> 2. $\E i,j\in S,i\neq j,\quad s.t.\;i|j$

解:
1. 构造鸽笼$\left\{1,2\right\},\left\{3,4\right\},\cdots,\left\{2n-1,2n\right\}$
2. 构造鸽笼$\left\{k,2k,4k,\cdots\right\}$, 其中$k$遍历奇数

>**Erdős–Szekeres定理 [参考此文](https://zhuanlan.zhihu.com/p/150750621)**
>
> 对于任意由 $n$ 个不同实数组成的序列，若 $n>rs$（$r,s$ 为正整数），则该序列中必然存在长度为 $r+1$的非升子序列，或长度为 $s+1$ 的非降子序列。

证明: 

 1. **定义二元组**： 对序列中的每个元素 $a_i$，定义二元组 $(r_i, s_i)$，其中：
	 - $r_i$ 是**以 $a_i$ 结尾的最长非升子序列的长度**；
	 - $s_i$ 是**以 $a_i$ 结尾的最长非降子序列的长度**。 
 2. **反证法假设**： 
    假设序列中**不存在长度为 $r+1$ 的非升子序列，也不存在长度为 $s+1$ 的非降子序列**，则对所有 $i$，有 $r_i \leq r$ 且 $s_i \leq s$。 
 3. **应用鸽笼原理**： 
    此时二元组 $(r_i, s_i)$ 的可能取值最多有 $r \times s$ 种（因为 $r_i \in \{1,2,\dots,r\}$，$s_i \in \{1,2,\dots,s\}$）。 但序列长度 $n > rs$，根据**鸽笼原理**：至少存在两个不同的元素 $a_p, a_q$（$p < q$），使得它们的二元组相等，即 $(r_p, s_p) = (r_q, s_q)$。 
 4. **导出矛盾**： 考虑 $a_p$ 和 $a_q$ 的大小关系： 
	 - 若 $a_p \geq a_q$：则以 $a_q$ 结尾的最长非升子序列长度应为 $r_p + 1$，即 $r_q = r_p + 1$，与 $r_q = r_p$ 矛盾； 
	 - 若 $a_p \leq a_q$：则以 $a_q$ 结尾的最长非降子序列长度应为 $s_p + 1$，即 $s_q = s_p + 1$，与 $s_q = s_p$ 矛盾。 

因此，原假设不成立，序列中必然存在长度为 $r+1$ 的非升子序列，或长度为 $s+1$ 的非降子序列。

# 生成函数

### 指数生成函数

$$\tilde{A}(x)=\sum_{n} \dfrac{1}{n!}a_{n}x^{n},~\left[\dfrac{x^{n}}{n!}\right]\tilde{A}(x)=a_{n}$$
常见:
$$\begin{align}
\left\{a_{n}\right\} \quad & \quad & \tilde{A}(x) \\
1,1,1,\cdots  &  & e^{x} \\
1,\alpha ,\alpha ^{2},\cdots  &  & e^{\alpha x} \\
0,1,2,3,\cdots  &  & xe^{x} \\  
\left\{S_{n}=n!\right\} &  &  S(x)=\dfrac{1}{1-x}\\
\left\{C_{n}=(n-1)!\right\} &  &C(x)= \ln \dfrac{1}{1-x} \\
1, \dfrac{1}{2}, \dfrac{1}{3}, \cdots  &  & \dfrac{e^{x}-1}{x} \\
\left\{\alpha ^{n}a_{n} \right\}  &  & \tilde{A}(\alpha x)
\end{align}$$

性质: $\left[\dfrac{x^{n}}{n!}\right](\tilde{A}(x)\tilde{B}(x))=\sum_{i=0}^{n} \binom nia_{i}b_{n-i}$
也就是$\left\{{\sum_{i=0}^{n}\binom nia_{i}b_{n-i}}\right\}$的指数生成函数是$\tilde{A}(x)\tilde{B}(x)$

$C(x)$ 长为n的轮换的个数
$\dfrac{C(x)^{2}}{2}$: 1~n恰分为两个轮换(不计次序)的方法数
...
$\dfrac{C(x)^{k}}{k!}$: 1~n恰分为k个轮换(不计次序)的方法数

$$S(x)=C(x)+ \dfrac{C(x)^{2}}{2}+\cdots=\sum_{k=1}^{+\infty} \dfrac{C(x)^{k}}{k!}=e^{C(x)}-1\xlongequal{差一个常数} \dfrac{1}{1-x}$$

# 概率

前情概要是一些或者trivial或者不care的东西. 

|                        | 记号                 | PMF(x)                                                  | $\mathbb{E}[X]$       | $Var(X)$                    | PGF                                                                            |
| ---------------------- | ------------------ | ------------------------------------------------------- | --------------------- | --------------------------- | ------------------------------------------------------------------------------ |
| 伯努利分布                  | Bern(p)            | $$\begin{cases}1-p&, x=0\\p&,x=1\end{cases}$$           | $p$                   | $p(1-p)$                    | $1-p+pz$                                                                       |
| 二项分布                   | Binom(n,p)         | $$\binom{n}{x}p^x(1-p)^{n-x}$$                          | $np$                  | $np(1-p)$                   | $(1-p+pz)^{n}$                                                                 |
| 均匀分布                   | Unif({a,...,b})    | $$\dfrac{1}{b-a+1},~x\in\{a,\dots,b\}$$                 | $$\dfrac{a+b}{2}$$    | $$\dfrac{(b-a+1)^2-1}{12}$$ | $\dfrac{z^{a}+\cdots+z^{b}}{b-a+1}$=$\dfrac{z^{a}(z^{b-a+1}-1)}{(b-a+1)(z-1)}$ |
| 几何分布<br>(首次成功的序号)      | Geom(p)            | $$(1-p)^{x-1}p,~ x\ge 1$$                               | $$\dfrac{1}{p}$$      | $$\dfrac{1-p}{p^2}$$        | $\dfrac{pz}{1-(1-p)z}$                                                         |
| 负二项分布<br>(r=1时加一为几何分布) | NBinom(r,p)        | $$\binom{r+x-1}{x}p^r(1-p)^{x}$$<br>$x\ge 0$            | $$\dfrac{r(1-p)}{p}$$ | $$\dfrac{r(1-p)}{p^2}$$     | $\left(\dfrac{p}{1-(1-p)z}\right)^{r}$                                         |
| 泊松分布                   | Poisson($\lambda$) | $$\dfrac{\lambda^x e^{-\lambda}}{x!}$$  $x=0,1,2,\dots$ | $\lambda$             | $\lambda$                   | $$e^{\lambda(z-1)}$$                                                           |

$\text{Poisson}(\lambda)=\lim_{ n \to \infty }\text{Binom}\left(n, \frac{\lambda}{n}\right)$, 可以验证:

$$
\begin{align}
\lim_{ n \to \infty } P_{\text{Binom}}(x) & =\lim_{ n \to \infty } \binom{n}{x}\left(\dfrac{\lambda}{n}\right)^{x}\left(1- \dfrac{\lambda}{n}\right)^{n-x} \\
 & =\lim_{ n \to \infty }  \dfrac{\cancel{n^{\underline{x}}} }{x!}\dfrac{\lambda^{x}}{\cancel {n^{x}}}\underbrace{\left(1- \dfrac{\lambda}{n}\right)^{n-x}}_{\approx e^{-\lambda}} \\
 & = \dfrac{e^{-\lambda}\lambda^{x}}{x!}
\end{align}$$

### 方差 (Variance)

- **定义**: 描述随机变量取值的分散程度。
  $$Var(X) = \mathbb{E}[(X - \mathbb{E}[X])^2] = \mathbb{E}[X^2] - (\mathbb{E}[X])^2$$

- **性质**:
  1.  $Var(c) = 0$ (c为常数)
  2.  $Var(X+c) = Var(X)$
  3.  $Var(cX) = c^2 Var(X)$
  4.  对于独立随机变量 $X, Y$: $Var(X+Y) = Var(X) + Var(Y)$
  5.  一般地: $Var(\sum_{i=1}^n X_i) = \sum_{i=1}^n Var(X_i) + 2\sum_{1 \le i < j \le n} Cov(X_i, X_j)$

**推论:** 一堆随机变量, 只要**两两独立**, 那么和的方差就等于方差的和

### 协方差 (Covariance)

- **定义**: 衡量两个随机变量的联合变化程度。
  $$Cov(X,Y) = \mathbb{E}[(X - \mathbb{E}[X])(Y - \mathbb{E}[Y])] = \mathbb{E}[XY] - \mathbb{E}[X]\mathbb{E}[Y]$$

- **性质**:
  1.  $Cov(X, Y) = Cov(Y, X)$
  2.  $Cov(X, X) = Var(X)$
  3.  $Cov(aX+b, cY+d) = ac \cdot Cov(X,Y)$
  4.  $Cov(X+Y, Z) = Cov(X,Z) + Cov(Y,Z)$
  5.  如果 $X, Y$ 相互独立, 则 $Cov(X,Y) = 0$ (反之不一定成立)

### 相关系数 (Correlation Coefficient)

- **定义**: 标准化后的协方差，衡量线性相关性强度。
  $$\rho_{XY} = \dfrac{Cov(X,Y)}{\sqrt{Var(X)Var(Y)}}$$
- **性质**:
  1.  $-1 \le \rho_{XY} \le 1$
  2.  $|\rho_{XY}|=1$ 的充要条件是 $X$ 和 $Y$ 之间存在线性关系, 即 $\E a,b\in\mathbb{R}, a\neq 0$, s.t. $Y=aX+b$
  3.  $\rho_{XY}=0$ 称为 $X,Y$ 不相关。独立必不相关，不相关不一定独立。

## 概率生成函数(\mathbf{Pr}obability Generating Function, PGF)

- **定义**:
$$G_X(z) = \mathbb{E}[z^X] = \sum_{k=0}^{\infty} P(X=k)z^k$$

- **性质**:
  1. $G_X(1) = \sum P(X=k) = 1$
  2. $Var(X) = G_X''(1) + G_X'(1) - (G_X'(1))^2$
  3. 若 $X, Y$ 独立, 则 $G_{X+Y}(z) = G_X(z)G_Y(z)$
  4. $G_{X}'(z)=\mathbb{E}\left[ \dfrac{\d}{\dz}x^{z}\right]=E[xz^{x-1}], ~G_{X}''(z)=\mathbb{E}\left[x(x-1)z^{x-2}\right]$
  5. $\mathbb{E}[X(X-1)\cdots(X-k+1)] = G_X^{(k)}(1)$, 特别地 $G_{X}'(1)=\mathbb{E}[X], ~G''_{X}(1)=\mathbb{E}[X(X-1)]\implies G_{X}''(1)+G_{X}'(1)=\mathbb{E}[X^{2}]$
  6. $P(X=k) = \dfrac{G_X^{(k)}(0)}{k!}$
  7. **矩生成函数 (MGF)**: $M_X(t) = G_X(e^t) = \mathbb{E}[e^{tX}]$. 它可以用来生成随机变量的各阶矩:
$$\mathbb{E}[X^k] = M_X^{(k)}(0) = \left.\dfrac{\d^k}{\dt^k}M_X(t)\right|_{t=0}$$
  8. **特征函数 (CF)**: $\varphi_X(t) = G_X(e^{it}) = \mathbb{E}[e^{itX}]$ (其中 $i$ 是虚数单位). 特征函数总是存在, 并且唯一确定了分布。它也可以用来求矩:$$\mathbb{E}[X^k] = \dfrac{\varphi_X^{(k)}(0)}{i^k}$$

### 条件期望

给定事件 $A$ 发生，$X$ 的条件期望为$\mathbb{E}[X|A] = \sum_{x} x P(X=x|A)$

 $\mathbb{E}[X|Y]$ 本身是一个随机变量，它的取值依赖于 $Y$ 的值。

- **全期望公式**:
  $$\mathbb{E}[X] = \mathbb{E}[\mathbb{E}[X|Y]]\left(  = \sum_{y} \mathbb{E}[X|Y=y]P(Y=y) \right)$$

- **其他性质**:
  1. **线性**: $\mathbb{E}[aX+bY|Z] = a\mathbb{E}[X|Z] + b\mathbb{E}[Y|Z]$
  2. **提取已知信息**: $\mathbb{E}[f(Y)X|Y] = f(Y)\mathbb{E}[X|Y]$
  3. **独立性**: 若 $X, Y$ 独立, 则 $\mathbb{E}[X|Y] = \mathbb{E}[X]$

### ~~条件方差~~

- **定义**:
  $$Var(X|Y) = \mathbb{E}[(X - \mathbb{E}[X|Y])^2 | Y]$$
  这是一个随机变量，其值取决于 $Y$。

- **全方差公式**:
  $$Var(X) = \mathbb{E}[Var(X|Y)] + Var(\mathbb{E}[X|Y])$$
  (方差 = 条件方差的期望 + 条件期望的方差)

### 一个例子

设 $X, Y : \Omega \to \mathbb{N}$，我们尝试描述
$Z = X_1 + \cdots + X_Y$。 $$\begin{align*} G_{\sum_{i=1}^Y X_i}(z) &= \mathbb{E}\left[z^{X_1} \cdots z^{X_Y}\right] \\ &= \mathbb{E}\left[\mathbb{E}\left[z^{X_1} \cdots z^{X_Y} \mid Y\right]\right] \\ &= \sum_y P_Y(y) \mathbb{E}\left[z^{X_1} \cdots z^{X_y} \mid Y = y\right] \\ &= \sum_y P_Y(y) \mathbb{E}\left[z^{X_1}\right] \cdots \mathbb{E}\left[z^{X_y}\right] \\ &= \sum_y P_Y(y) G_X(z)^y \\ &= G_Y(G_X(z)). \end{align*}$$

# 熵

$H[X]=\sum_{x}P_{X}(x)\log \dfrac{1}{P_{X}(x)}$
这里不规定$\log$的底数, 如果是$2$, 那么单位为bit; 如果是$e$, 那么单位为nit(奈特)

$H[X]=\mathbb{E}_{X\sim P_{X}} \left[\log \dfrac{1}{P_{X}(X)}\right]$, 注意区分两个$X$的含义, 一个是抽象的, 一个是取值实例

### 联合熵

$H[X,Y]=\sum_{x,y}P_{X,Y}(x,y)\log \dfrac{1}{P_{X,Y}(x,y)}=\mathbb{E}_{(X,Y)\sim P_{X,Y}}\left[\log \dfrac{1}{P_{X,Y}(X,Y)}\right]$

它衡量的是一对随机变量$(X,Y)$整体的不确定性。

### 条件熵

使用$P_{XY}(x,y)=P_{X}(x)P_{Y|X}(y|x)$可得

$$\begin{align}
H[Y|X]  & = \sum_{x} P_X(x) H[Y|X=x]  \\
 & = \sum_{x} P_X(x) \left[ \sum_y P_{Y|X}(y|x) \log \frac{1}{P_{Y|X}(y|x)} \right] \\
 & =\sum_{x,y} P_{X,Y}(x,y) \log \frac{1}{P_{Y|X}(y|x)} \\
 & =\mathbb{E}_{(X,Y)\sim P_{X,Y}}\left[\log \frac{1}{P_{Y|X}(Y|X)}\right]
\end{align}$$

它衡量的是在已知随机变量 $X$ 的情况下，随机变量 $Y$ 的剩余不确定性。

> 熵的链式法则
> $$H[X,Y] = H[X] + H[Y|X]$$
> 证:
> 展开验证不难

> 条件熵
> $$H[X|Y]\leqslant H[X]$$
> 证:
> 琴生不等式

### 互信息

$$\begin{align}
I(X,Y)  :&=H[X]-H[X|Y] \\
 & =H[Y]-H[Y|X] \\
 & =H[X]+H[Y]-H[X,Y]
\end{align}$$

可以作韦恩图
![[6f9d8121-58d9-4814-b205-1e552e3ee272.png|300]]

考虑三个随机变量$X,Y,Z$, 一般地有$P_{XYZ}=P_{X}P_{Y|X}P_{Z|XY}$

假设有$P_{XYZ}=P_{X}P_{Y|X}P_{Z|Y}$(类似于一个逐步生成后一项的机器), 那么可以利用$P_{XY}=P_{X}P_{Y|X}$推出事实上有$$P_{XYZ}=P_{X}P_{Y|X}P_{Z|Y}=P_{Y}P_{X|Y}P_{Z|Y}=P_{Z}P_{Y|Z}P_{X|Y}$$
分别代表$X\to Y\to Z,~X\leftarrow Y\to Z,~Z\to Y\to X$ 

> 命题
> 此时($P_{XYZ}=P_{X}P_{Y|X}P_{Z|Y}$)有$I(X,Y)\leqslant I(X,Z)$


下面我们开始考虑两个不同的分布$P,Q$:

统计距离"the statistical distance":$$\sum_{x} \dfrac{\left|P(x)-Q(x)\right|}{2}=\sum_{x}\max(0,P(x)-Q(x))$$
### KL散度(KL Divergence)

$$\begin{align}
D(P\|Q) & =\sum_{x} P(x) \log \frac{P(x)}{Q(x)} \\
 & =\mathbb{E}_{X\sim P} \left[ \log \frac{P(X)}{Q(X)} \right] \\
 & =\mathbb{E}_{X\sim Q} \left[ \frac{P(X)}{Q(X)} \log \frac{P(X)}{Q(X)} \right]
\end{align}$$

如果$support(P)\nsubseteq support(Q)$, 上式为$+\infty$

> 性质
> $$D(P\|Q)\geqslant 0$$
> 证: 


> $$D(P\|\mathrm{Unif}(\Omega))=\log \left|\Omega\right|-H(P)$$
> 证:

我们考虑多变量的情况$D(P_{XY}\|Q_{XY})$. 

我们首先定义条件散度
$$D(P_{Y|X}\|Q_{Y|X}|P_{X}):=\sum_{x} D(P_{Y|X=x}\|Q_{Y|X=x})P_{X}(x)$$
 基于此定义$$D(P_{XY}\|Q_{XY}):=D(P_{X}\|Q_{X})+D(P_{Y|X}\|Q_{Y|X}|P_{X})$$
# 集中不等式

### 马尔可夫不等式

>随机变量$X~over[0,+\infty)$,  $$\mathbf{Pr}[X\geqslant \alpha \mathbb{E}[X]]\leqslant  \dfrac{1}{\alpha }\quad\text{或曰}\quad\mathbf{Pr}[X\geqslant \beta]\leqslant \dfrac{\mathbb{E}[X]}{\beta}$$

事实上, 若以$x$为横轴, $\mathbf{Pr}[X\geqslant x]$为纵轴, 则这个单减趋于0的函数的曲线下面积就是$\mathbb{E}[X]$

进而一定大于所有的矩形, 取$x=\alpha \mathbb{E}[x]$也即$$\alpha \mathbb{E}[x]*\mathbf{Pr}[X\geqslant \alpha \mathbb{E}[x]]\leqslant \mathbb{E}[x]\implies \mathbf{Pr}[x\geqslant \alpha \mathbb{E}[X]]\leqslant  \dfrac{1}{\alpha }$$
### 切比雪夫不等式

>随机变量$X$, $$\mathbf{Pr}\left[\left|X-\mathbb{E}[X]\right|\geqslant \alpha \sqrt{\text{Var}[X]}\right]\leqslant  \dfrac{1}{\alpha ^{2}}$$

由于$\text{Var}[X]=\mathbb{E}\left[\left(X-\mathbb{E}[X]\right)^{2}\right]$, 使用马尔可夫不等式, $$\mathbf{Pr}\left[\left|X-\mathbb{E}[X]\right|\geqslant \alpha \sqrt{\text{Var}[X]}\right]=\mathbf{Pr}\left[\left(X-\mathbb{E}[X]\right)^{2}\geqslant \alpha ^{2}{\text{Var}[X]}\right]\leqslant  \dfrac{1}{\alpha ^{2}}$$
### ## 切诺夫界 Chernoff Bound

> 随机变量$X$,$$\operatorname{\mathbf{Pr}}\left[X\geqslant \alpha \right]\leqslant \min_{t>0} \dfrac{\mathbb{E}\left[e^{tX}\right]}{e^{t\alpha }}$$



独立同分布$\many{X}\sim Bern(p)$, $q>p$

$$\begin{align}
\mathbf{Pr}\left[ \sum X_{i}\geqslant qn \right] & \leqslant \dfrac{p}{q} &  & \cdots\text{马尔可夫} \\
 & \leqslant  ~? &  & \cdots\text{切比雪夫} \\
 & \leqslant \exp(-d(q\|p)*n)=\exp(D(\operatorname{Bern}(q)\|\operatorname{Bern}(p))*n) &  & \cdots 
\end{align}$$



# 离散傅里叶变换

**定义Character**: 有限阿贝尔群$G\to \mathbb S^{1}\left(=\left\{z\in \mathbb{C}\big{\vert}\left|z\right|=1\right\}\right)$的同态 (其实就是到$\mathbb{C}$的同态, 因有限而必然在单位圆上)

回顾同态的定义有$\chi(x)\chi(y)=\chi(x+y)$

记$n:=\left|G\right|$

对于群$G=\mathbb{Z}_{n_{1}}\times\mathbb{Z}_{n_{2}}\times\cdots\times\mathbb{Z}_{n_{k}}$以及$a\in G$
定义Character $\chi_{a}(x)=\omega_{n_{1}}^{a_{1}x_{1}}\cdot\omega_{n_{2}}^{a_{2}x_{2}}\cdots\omega_{n_{k}}^{a_{k}x_{k}}$, 其中$a_{i},x_{i}$指的是第$i$分量, $\omega_{n}$是$n$次单位根. 

显然有$\chi_{a}(b)=\chi_{b}(a)$

不难看出$\chi_{a}\chi_{b}=\chi_{a+b}$, 因此$\left\{\chi_{a}|a\in G\right\}$是一个群.

**性质** $$\sum_{x}\chi_{a}(x)=\sum_{(\many[k]{x})}\prod_{j}\omega_{n_{j}}^{a_{j}x_{j}}=\prod_{j}\sum_{x_{j}\in Z_{n_{j}}}\omega_{n_{j}}^{a_{j}x_{j}}=\prod_{j}~n_{j}~\delta_{a_{j},0}=\begin{cases}
n & ,a=\vec0 \\
0 & ,else
\end{cases}$$

定义复数内积$\left\langle f,g \right\rangle=\sum_{x}f(x)\overline{g(x)}$

在单位圆上, $\overline{\chi_{a}}=\chi_{-a}=\chi_{a}^{-1}$ **这里的$^{-1}$指的是单位圆上的乘法逆, 不是函数的逆!

因此$$\left\langle \chi_{a},\chi_{b} \right\rangle =\sum_{x}\chi_{a}(x)\overline{\chi_{b}(x)}=\sum_{x}\chi_{a}(x)\chi_{-b}(x)=\sum_{x}\chi_{a-b}(x)=\begin{cases}
n & ,a=b \\
0 & ,a\neq b
\end{cases}$$

也即**这些$\chi_{a}$两两正交, 又由于个数为$\left|G\right|$, 因此已经构成空间$\left\{f:G\to \mathbb{C}\right\}$一组正交基**

下面证明

> **命题**
> Character只有$\chi_{a}$

对于任意Character $\chi$, 我们尝试证明引理$$\sum_{x}\chi(x)=\begin{cases}
n& ,\chi=\chi_{\vec{0}},~或曰\chi全1 \\
0 & ,else
\end{cases}$$
对于非零Character, $\E x^{*}, \chi(x^{*})\neq1$

那么$\chi(x^{*})\sum_{x}\chi(x)=\sum_{x}\chi(x+x^{*})=\sum_{x}\chi(x)\implies \sum_{x}\chi(x)=0$, 引理证毕

对于任意两个Character, $$\left\langle \chi,\chi' \right\rangle=\sum_{x}\chi(x)\chi' (x)^{-1}=\sum_{x}(\chi \chi'^{-1})(x)=\begin{cases}
n & ,\chi=\chi' \\
0 & ,else
\end{cases}$$


因此所有Character两两正交, 结合$\left\{\chi_{a}\right\}$已经构成正交基, 因此$\left\{\chi_{a}\right\}$就是全体Character. 

另一种证明: 由于群同态, 群元素(0,...,0,1,0,...)必然被映射到$\omega_{n_{i}}^{k_{i}}$, 因此收集这些$k_{i}$即可. 

### 离散傅里叶

$f:G\to\mathbb{C}$, $f=\sum \hat{f(b)}\chi_{b}$,  内积可以得到$\hat{f}(b)=\frac{1}{n}\left\langle f,\chi_{b} \right\rangle$

定义$\|f\|_{2}^{2}=\left\langle f,f \right\rangle=\sum_{x}\left|f(x)\right|^{2}$

因此$$\begin{align}
\|f\|_{2}^{2} & =\left\langle \sum\hat{f(a)}\chi_{a}, \sum\hat{f(b)}\chi_{b} \right\rangle  \\
 & = \sum \left|\hat{f}(b)\right|^{2}n \\
 & =n \|\hat{f}\|^{2}_{2}
\end{align}$$

**性质**: 逆傅里叶变换也是"傅里叶变换", 即, $f$和$\hat{f}$是对偶的, 具体地:

$$\begin{align}
&\left\langle \hat{f},\chi_{y} \right\rangle =\sum_{b}\hat{f(b)}\chi_{y}(b)^{-1}=\sum_{b} \dfrac{1}{n} \sum_{x}f(x)\chi_{b}(x)^{-1}\chi_{y}(b)^{-1}=\dfrac{1}{n}\sum_{b,x}f(x)\left(\chi_{b}(x)\chi_{b}(y) \right)^{-1} \\  
 & =\dfrac{1}{n} \sum_{b,x}f(x)\chi_{b}(x+y)^{-1}=\dfrac{1}{n} \sum_{b,x}f(x)\chi_{x+y}(b)^{-1}=\dfrac{1}{n}\sum_{x}f(x)\sum_{b}\chi_{x+y}(b)^{-1} \\
 & \xlongequal{只有x+y=0存活} \dfrac{1}{n}f(-y)n=f(-y)
\end{align}$$


(为什么是||f||^2=n||hatf||^2显然的?)

另一种傅里叶变换:
$f:G\to\mathbb{C}, \hat{f}(a)=\left\langle f,\chi_{a} \right\rangle$, 差一个n倍, 因此此时有$n\|f\|_{2}^{2}=\|\hat{f}\|^{2}$

### 卷积 

定义: $(f*g)(x)=\sum_{y}f(y)g(x-y)$

### 傅里叶系数的运算

- 加法
$\widehat{(cf+g)}=c\hat{f}+\hat{g}$

- 乘法
$f\cdot g=\sum_{a,b}\hat{f}(a)\hat{g}(b)\chi_{a}\chi_{b}=\sum_{a,b}\hat{f}(a)\hat{g}(b)\chi_{a+b}$
因此 $\widehat{f\cdot g}=\sum_{a+b=c}\hat{f}(a)\hat{g}(b)=\hat{f}*\hat{g}$

- 卷积
$\widehat{f*g}(a)=\dfrac{1}{n} \left\langle f*g,\chi_{a} \right\rangle=$
因此 $\widehat{f*g}=n\cdot\hat{f}\cdot \hat{g}$


### 与概率密度函数

$P_{X}, P_{Y}:G\to \mathbb{R}$, 那么$P_{X+Y}(z)=\operatorname{\mathbf{Pr}}\left[X+Y=z\right]=(P_{X}*P_{Y})(z)$

对于独立同分布变量$\many[k]{X}$, $$P_{\sum X_{i}}=\underbrace{P_{X}*P_{X}*\cdots*P_{X}}_{k~times}$$
怎么计算? 离散傅里叶

$\widehat{P_{\sum X_{i}}}=n^{k-1}(\hat{P}_{X})^{t}$

### 布尔函数分析

考虑函数$f:\left\{0,1\right\}^{n}\to \left\{0,1\right\}$

(或者我们也会把值域写成$\left\{1,-1\right\}=\left\{(-1)^{0},(-1)^{1}\right\}$, 因此二者事实上是类似的, 且有$f'=(-1)^{f}=1-2f$ )

此时Character也简化成 $\chi_{a}(x)=\prod(-1)^{a_{i}x_{i}}=(-1)^{\left\langle a,x \right\rangle}$

对于一个函数$f$, 我们定义记号$f_{A\leftarrow \alpha}$, 其中$A\subseteq [n], \alpha:A\to \left\{0,1\right\}$, 表示把输入$x$中$A$的那些为强行设置成$\alpha$后再输入$f$的结果. 也即$$f_{A\leftarrow \alpha }(x)=f(x_{A\leftarrow \alpha }), ~where~x_{A\leftarrow \alpha }[i]=\begin{cases}
x[i] & ,i\notin A \\
\alpha (i) & ,i\in A
\end{cases}$$
计算傅里叶系数: 

先取简单情况, $A={1},\alpha\in \left\{0,1\right\}$
$$\mathbb{E}_{\alpha }\left[\widehat{f_{A\leftarrow \alpha } (a)}\right]=\begin{cases}
0 & ,a_{1}=1 \\
\hat{f}(a) & ,a_{1}=0
\end{cases}$$

进一步地, fix $A\subseteq [n]$, $$\mathbb{E}_{\alpha }\left[\widehat{f_{A\leftarrow \alpha } (a)}\right]=\begin{cases}
0 & ,\E i\in A,~a_{i}=1 \\
\hat{f}(a) & ,\F i\in A,~ a_{1}=0
\end{cases}$$
更进一步地, 随机取A, s.t. $\operatorname{\mathbf{Pr}}\left[i\in A\right]=p$且各分量独立. $$\mathbb{E}_{\alpha }\left[\widehat{f_{A\leftarrow \alpha } (a)}\right]=p^{a中1的个数}\hat{f}(a)$$

# 编码

回忆$$\begin{align}
 & H(X)=\mathbb{E}\left[\log \dfrac{1}{P(X)} \right] \\
 & H_{min}(X):=\log \dfrac{1}{\max_{x}P(x)} \\
 & H_{max}(X):=\log \left|support~of~X\right|  \\
 & H_{2}(X):=\log \dfrac{1}{P_{X,Y\sim P}(X=Y)}=\log\left( \sum_{x}P^{2}(x) \right)
\end{align}$$

编码: 对于$X\sim P$, 考虑编码函数$E$, 将之映射成一个01字符串$c$, 以及对应的解码函数$D$, 满足:

1. $D(E(x))=x$
2. $\mathbb{E}\left[\operatorname{len}(c)\right]$尽可能小

显然最优的方法是把概率最大的映射成空集, 次大和第三大的映射成0,1; 第$2^{n-1}\sim2^{n}$大的映射成长为$n$的01串. 

此时可以证明$$H(X)-H(L)\leqslant \mathbb{E}\left[\len(X)\right]\leqslant H(X)$$并且$H(L)$很小, 具体地, 小于$\log_{2}(e(1+H(X)))$.


下面解决歧义问题, 此时要求编码器$E$满足$$f^{*}:A^{*}\to \left\{0,1\right\}^{*},\quad s.t.\;f(a_{1}a_{2}\cdots a_{n})\xlongequal{唯一}f(a_{1})f(a_{2})\cdots f(a_{n})$$
比如 prefix-free code, 必然满足上述要求

> **性质** 如果固定了每个$a$对应的字符串的长度$L:A\to \mathbb{N}$, 则存在满足要求的 prefix-free code 的充要条件是$$\sum_{a\in A}2^{-L(a)}\leqslant 1$$

> Huffman编码是最优的前缀码.

证: 使用归纳法, 只用证明一定存在一个最优的前缀编码把最小的两个数"绑在一起". 否则, 考虑他俩中深度更深的那一个, 把他的兄弟节点连同下面的子树与他俩中深度更浅的那一个交换. 此时他俩变成兄弟且整体情况不会变劣. 证毕



# 马尔科夫链

一组随机变量$X_{0},X_{1},X_{2}\cdots$, 定义**马尔科夫性质(markov property)**$$Given~X_{i},~X_{i+1}~and~(X_{0},\many[i-1]{X})~独立$$
此时联合分布可以写成$$P_{X_{0}}P_{X_{1}|X_{0}}P_{X_{2}|X_{1}}\cdots $$

我们今天考虑更特殊的马尔科夫链, 满足**time-homogeneous(时间齐次(或简称时齐))**:$$P_{X_{1}|X_{0}}=P_{X_{2}|X_{1}}=P_{X_{3}|X_{2}}=\cdots=:P$$
并且每个随机变量的空间都是$\Omega$.

因此, $P(y|x)\xlongequal{\triangle}P(x,y)$事实上是$P:\Omega \times\Omega\to[0,1]$, 可以表示成一个矩阵$$\begin{pmatrix}
P(a_{i},a_{j})
\end{pmatrix}_{i,j}$$
满足每个位置都非负且每一行的和为1.

还有一些记号$$\begin{align}
\operatorname{\mathbf{Pr}}_{x}\left[\cdots\right] & =\operatorname{\mathbf{Pr}}\left[\cdots|X_{0}=x\right] \\
\mathbb{E}_{x}\left[\cdots\right] & =\mathbb{E}\left[\cdots|X_{0}=x\right]
\end{align}$$
--- 

定义完毕, 现在开始~~玩弄~~研究这个矩阵. 

假设$\mu$是一个分布(视为行向量), 那么$\mu P$就是当$X_{t}\sim \mu$时, $X_{t+1}$的分布.

假设$f$是一个函数(视为列向量), 那么$(Pf)(x)=\mathbb{E}\left[f(X_{t+1})|X_{t}=x\right]=\mathbb{E}_{x}\left[f(X_{1})\right]$, ($Pf$是矩阵和向量的乘法)

$P^{t}(x,y)=\operatorname{\mathbf{Pr}}_{x}\left[X_{t}=y\right]$

如果$\pi$是一个满足$\pi P=\pi$的分布, 则称之为**稳态分布**

**Hiting time** $\tau_{x}=\begin{cases}\min\left\{t\geqslant0,\quad s.t.\;X_{t}=x\right\} \\ \infty & ,if ~\F t\geqslant0,X_{t}\neq x\end{cases}$

**Return time** $\tau_{x}^{+}=\begin{cases}\min\left\{t>0,\quad s.t.\;X_{t}=x\right\} \\ \infty & ,if ~\F t>0,X_{t}\neq x\end{cases}$

---

我们研究这些特殊的马尔科夫链:
1. time-homogeneous
2. finite: $\left|\Omega\right|<+\infty$
3. irreducible: $\F x,y\in\Omega, x可达y$. 可达的定义: $\E t,P^{t}(x,y)>0$

> **性质** 上面三个性质满足, 则$\F x,y\in\Omega, \mathbb{E}_{x}\left[\tau_{y}\right]和\mathbb{E}_{x}\left[\tau_{y}^{+}\right]均<+\infty$.
> **证明** 只需证后者$<+\infty$. 考虑任意$z\in\Omega$, 根据irreducible, $\E t_{z}>0, P^{t_{z}}(z,y)>0$. 取$\max_{z}t_{z}=: t$, $\min_{z}P^{t_{z}}(z,y)=:p$ 那么$\F x,\operatorname{\mathbf{Pr}}_{x}\left[\tau_{y}^{+}> t\right]\leqslant1-p$. 
> 归纳$$\operatorname{\mathbf{Pr}}_{x}\left[\tau_{y}^{+}> nt\right]=\operatorname{\mathbf{Pr}}_{x}\left[\tau_{y}^{+}> (n-1)t\right] \operatorname{\mathbf{Pr}}_{x}\left[\tau_{y}^{+}> nt|\tau_{y}^{+}> (n-1)t\right]\leqslant(1-p)^{n}$$
> 因此代入立得期望有限.

> **性质** 上面三个性质满足, 则稳态分布存在.
> **证明** fix $x\in\Omega$, 定义$$\begin{align}
\tilde{\pi}(y) & =\mathbb{E}_{x}\left[\#visit ~ to~ ~y~ ~before ~returning ~to~ ~x\right] \\
 & =\mathbb{E}_{x}\left[\left|\left\{t|X_{t}=y, 0\leqslant t<\tau_{x}^{+}\right\}\right|\right] \\
 &  = \sum_{t}\operatorname{\mathbf{Pr}}_{x}\left[X_{t}=y~\&~0\leqslant t<\tau_{y}^{+}\right]
\end{align}$$特别地, $\tilde{\pi}(x)=1$. 
> 关于其性质, 显然有$\sum_{y}\tilde{\pi}(y)=\mathbb{E}_{x}\left[\tau_{x}^{+}\right]$. 
> 我们证明$\pi(y)= \dfrac{\tilde{\pi}(y)}{\mathbb{E}_{x}\left[\tau_{x}^{+}\right]}$满足题意. 也即$\tilde{\pi}P=\tilde{\pi}$.
> $$\begin{align}
\tilde{\pi}(y) & =\sum_{t=1}^{+\infty}\operatorname{\mathbf{Pr}}_{x}\left[X_{t}=y~\&~0< t\leqslant \tau_{x}^{+}\right] \\
 & = \sum_{t=1}^{+\infty}\sum_{z}\Pr[x]{X_{t}=y|X_{t-1}=z~\&~0<t-1<\tau_{x}^{+}}\Pr[]{X_{t-1}=z~\&~0<t\leqslant \tau_{x}^{+}} \\
 & =\sum_{z}P(z,y)\sum_{t=1}^{+\infty}\Pr[]{X_{t-1}=z~\&~0<t\leqslant \tau_{x}^{+}} \\
 & =\sum_{z}P(z,y)\sum_{t=0}^{+\infty}\Pr[]{X_{t}=z~\&~0\leqslant t< \tau_{x}^{+}} \\
 & =\sum_{z}P(z,y)\tilde{\pi}(z) \\
 & =(\tilde{\pi}P)(y)
\end{align}$$

> **性质** 事实上稳态分布的存在性不需要irreducible, 但是 唯一性$\Longleftrightarrow$irreducible.
> **证明** 假设存在不同的稳态分布$\pi,\pi'$, 记$\mu:=\pi-\pi',=> \mu P=\mu$. 且$\mu$有正有负. 
> 此时有
> $$\|\mu\|_{1}=\sum_{y}|\mu(y)|=\sum_{y}\left|\sum_{x}\mu(x)P(x,y)\right|\leqslant \sum_{y}\sum_{x}|\mu(x)|P(x,y)=\sum_{x}|\mu(x)|\sum_{y}P(x,y)=\|\mu\|_{1}$$
> 等号成立当且仅当对于任意$y$, 所有满足$P(x,y)>0$的$\mu(x)$同号. 
> 这意味着无法从$\mu(x)>0$的状态一步走到$\mu(y)<0$的状态(否则在$y$处会有异号相加, 导致严格不等式). 
> 这说明正集合$S_{+}=\{x|\mu(x)>0\}$无法到达负集合$S_{-}=\{x|\mu(x)<0\}$, 与不可约性矛盾. 
> 
> **证明2** 考虑$P-I$, 那么只需说明其秩为$\left|\Omega\right|-1$. 我们把向量放在另外一边, 也就是要说明关于$f$的方程$(P-I)f=0$的解空间为$1$.
> 
> 定义**和谐的**: 函数$h$在$x$处和谐, if $\mathbb{E}_{x}\left[h(X_{1})\right]=h(x)$, 也即$h(x)=\sum_{y}P(x,y)h(y)=(Ph)(x)$.
> 如果$h$在每个$x$上都和谐, 则简称称$h$和谐. $\Longleftrightarrow~h=Ph$, 也即前文的$f$. 
> 
> 记$\alpha=\max_{x}h(x)$, 并以$x$称呼这个最大的$x$. 由性质可知 $\F y,\E t, P^{t}(x,y)>0$. 于是$$\alpha =h(x)=(P^{t}h)(x)=\sum h(z)P^{t}(x,z)$$左侧是最大值, 右侧是加权平均, 二者相等说明$h$只能各个位置上的数相等, 也即其解空间是1. 证毕

根据前面的构造, 这个唯一的稳态分布就是$\pi(x)=\dfrac{1}{\mathbb{E}_{x}\left[\tau_{x}^{+}\right]}$

继续研究马尔科夫链的另一个性质period/aperiod:

$x$处的period$:=\gcd\left(\left\{t|P^{t}(x,x)>0\right\}\right)$. 如果为1, 则称在x处无周期(aperiod)

如果$\F x,$ aperiod. 则称这个马尔科夫链无周期(aperiod)

> 下面证明, 前面的三个性质+无周期性$\implies$收敛性(convergence), 也即 $\E \pi\F 分布\mu,~\lim_{ t \to \infty }\mu P^{t}=\pi$.
> **证明** 无周期性结合另外的性质能够够告诉我们$\E n, \F x,y, P(x,y)>0$,
> 因此定义一组$c_{x}>0$,使得 $\F x,y,(\delta_{x}P^{n})(y)\geqslant \pi(y)c_{x}$.
> 令 $c = \min_{x} c_{x} > 0$.
> 则对于任意初始分布 $\mu$, 有
> $$(\mu P^{n})(y) = \sum_{x}\mu(x)(\delta_{x}P^{n})(y) \geqslant \sum_{x}\mu(x)c\pi(y) = c\pi(y)$$
> 这意味着我们可以把 $\mu P^{n}$ 拆分:
> $$\mu P^{n} = c\pi + (1-c)\mu'$$
> 其中 $\mu' = \frac{\mu P^{n}-c\pi}{1-c}$ 也是一个分布 (易证非负且和为1).
> 
> 迭代此过程:
> $$\begin{align}
\mu P^{2n} & = (\mu P^{n})P^{n} = (c\pi + (1-c)\mu')P^{n} \\
 & = c\pi P^{n} + (1-c)\mu' P^{n} \\
 & = c\pi + (1-c)(c\pi + (1-c)\mu'') \\
 & = (1-(1-c)^{2})\pi + (1-c)^{2}\mu''
\end{align}$$
> 归纳可得 $\mu P^{kn} = (1-(1-c)^{k})\pi + (1-c)^{k}\mu^{(k)}$.
> 当 $k\to \infty$ 时, $(1-c)^{k}\to 0$, 故 $\mu P^{kn} \to \pi$.
> 进而由于无周期性, $\mu P^{t} \to \pi$.


我们来看几个例子.

### 一维随机游走

> **一维随机游走** 初始时在$k$处, 每次各以$\dfrac{1}{2}$的概率向左/向右一步, 直至到达$0$或$n$.
> **不平衡的一维随机游走** 向右概率$p$, 向左概率$1-p$

我们研究以下三个量:
1. $\Pr[]{\E t,X_{t}=0}$.
2. $\Pr[]{\E t,X_{t}=n}$.
3. $\tau=\min\left\{t|X_{t}=0~or~n\right\}$

考虑一维随机游走
结合递推式和边界容易说明$\Pr[]{\E t,X_{t}=n}=\dfrac{k}{n}=1-\Pr[]{\E t,X_{t}=n}$
另一种想法是, 这个游戏的期望收益是$0$, 因此为保持期望不变, 可以直接解出.

同样结合递推式和边界, $\mathbb{E}_{k}\left[\tau\right]=k(n-k)$


### Coupon Collection

> 你每次收集一个编号均匀分布在$1\sim n$的优惠券, 何时才能集齐所有优惠券?

一方面, 视之为关于"已有优惠券总类别数"的马尔科夫链

容易计算$\mathbb{E}_{0}\left[\tau\right]=n\sum_{i=1}^{n} \dfrac{1}{i}$

更精细的, 我们考虑$$\begin{align}
\Pr[]{\tau\geqslant n\ln n+\varepsilon n} & \leqslant \sum_{i=1}^{n}\Pr[]{i在n\ln n+\varepsilon n时仍未被收集} \\
 & = n\left(1- \dfrac{1}{n}\right)^{n\ln n+\varepsilon n} \\
 & \approx \dfrac{n}{e^{\ln n+\varepsilon}} \\
 & =e^{-\varepsilon}
\end{align}$$


### 超立方体上的随机游走

> 也就是随机把$s\in \left\{0,1\right\}^{n}$上的一位反转

> 也就是在两个瓶子中装$n$个互异的球, 每次随机取一个球放入另一个瓶子

显然稳态是均匀分布, 且具有周期2. 去周期的方法:$P\overset{\text{lazy}}{\longrightarrow} \dfrac{I+P}{2}$, 在超立方体的例子中, 这等价于"选一位, 然后把他换成随机值"

### 生灭过程

> 状态$0\sim n$, 且只往相邻状态转移或不变.
> 记$P(k,k+1)=p_{k}, ~P(k,k-1)=q_{k},~P(k,k)=1-p_{k}-q_{k}$

irreducible$\Longleftrightarrow$每个点向每一侧的概率都非零
aperiod$\Longleftrightarrow$存在一个点不变的概率非零

**Detail Balanced** 

我们考虑其稳态分布$\pi$, 根据定义$\pi(k)=\sum_{y}\pi(y)P(y,x)=\sum_{y}\pi(x)P(x,y)\implies \sum_{y\neq x}\pi(y)P(y,x)=\sum_{y\neq x}\pi(x)P(x,y)$

我们希望更强的东西, 即**Detail Balanced**: $\F x,y,~ \pi(y)P(y,x)=\pi(x)P(x,y)$

事实上, 生灭过程的稳态分布必为**Detail Balanced**. 因为我们可以在$k$和$k+1$中间划分, 左侧的权重和和右侧的权重和都保持不变, 也即"流动量"一样, $\pi(k)P(k,k+1)=\pi(k+1)P(k+1,k)$

### 图上的随机游走

> 每一步随机的走到一个邻居, 也即
> $$P(x,y)=\begin{cases}
0 & ,(x,y)\notin E \\
\dfrac{1}{\deg(x)} & ,(x,y)\in E 
\end{cases}$$

### 整数上的随机游走

> 从$k$出发, $P(n,n\pm1)=\dfrac{1}{2}$. 或其lazy版本

记$\tau_{0}=\min\left\{t|X_{t}=0\right\}$

> **性质** $\Pr[]{\tau_{0}<+\infty}=1$. 
> **证明** 考虑从$k$到$0~or~2k$. 有至少1/2概率. 否则再考虑从$2k$到$0~or~4k$, 以此类推...

> **但是** $\mathbb{E}_{k}\left[\tau_{0}\right]=+\infty$

> **性质** $k,t,x\geqslant0$, $\Pr[k]{X_{t}=x~\&~\tau_{0}<k}=\Pr[k]{X_{t}=-x}$


### MCMC

我们想要依照$\mu$选点. 但是我们不知道$\mu$的样子, 甚至可能不是一个分布(仅仅希望$P(x)\sim \mu(x)$)

> start from $x\in\left\{0,1\right\}^{n}$, sample 一个随机邻居$y$, w.p. $\alpha(x,y)$ 待定, update $x$ to $y$, 否则不变.

我们希望$\mu$是这样的马尔科夫链的稳态

利用**Detail Balanced**. 也即我们希望$\mu(x) \dfrac{\alpha(x,y)}{n}=\mu(y) \dfrac{\alpha(y,x)}{n}$. 

可以让$\alpha(x,y)=\min\left(1, \dfrac{\mu (y)}{\mu(x)}\right), \alpha(y,x)=\min\left(1, \dfrac{\mu (x)}{\mu(y)}\right)$

实际上不需要在超立方体上进行迭代, 而是可以基于一个已有的马尔科夫链$\psi$, (配备已有的**Detail Balanced** 分布$\pi$).

此时我们利用**Detail Balanced** 希望$$\mu(x)\psi(x,y)\alpha (x,y)=\mu(y)\psi(y,x)\alpha (y,x)$$
由于$\pi$之于$\psi$是detail balanced, 因此可以取$\alpha(x,y)=\min\left(1, \dfrac{\mu(y)\pi(x)}{\mu(x)\pi(y)}\right)$

### Mixing Time

定义$d(t)=\sup_{\mu}\Delta(\mu P^{t},\pi)$, $\bar{d}(t)=\sup_{\mu,\nu}\Delta(\mu P^{t},\nu P^{t})$.

显然地有$d(t)\leqslant \bar{d}(t)\leqslant2d(t)$. 不难证明的还有这两个函数都是不严格单调递减的

定义mixtime $t_{mix}(\varepsilon)=\min\left\{t|d(t)\leqslant\varepsilon\right\}$

我们聚焦lazy超立方体游走, 由于, 一旦遍历完所有的位, 那么分布一定随机, 因此有$$d(n\ln n+\varepsilon n)\leqslant e^{-\varepsilon}$$



### Coupling

有两个分布$P_{x},P_{y}$, 我们关心$\Delta(P_{x},P_{y})$, 显然他小于等于$\Pr[]{X\neq Y}$.

我们现在关心取等条件. 

> **Coupling Lemma**
>
> $$\Delta(P_x, P_y) = \min_{(X,Y)\text{ is a coupling}} \Pr[]{X \neq Y}$$
>
> 即存在一个 coupling $(X,Y)$ 使得 $\Pr[]{X \neq Y} = \Delta(P_x, P_y)$.

**构造最优 Coupling**:
设 $p(z) = P_x(z), q(z) = P_y(z)$.
1. 保持 $X=Y=z$ 的概率为 $\min(p(z), q(z))$. 这部分的概率和为 $\sum_z \min(p(z), q(z)) = 1 - \Delta(P_x, P_y)$.
2. 剩下的概率部分, $X$ 按照 $\frac{(p(z)-q(z))^+}{\Delta(P_x, P_y)}$ 分布, $Y$ 按照 $\frac{(q(z)-p(z))^+}{\Delta(P_x, P_y)}$ 分布, 且 $X, Y$ 独立.
此时 $X \neq Y$ 必然发生 (因为支撑集不交).

下面举一个例子: 

考虑环上的lazy随机游走的mixing time, ( lazy 游走 on $\mathbb{Z}_n$), 使用coupling方法, 我们可以考虑两个马尔科夫链. 定义分布$$P:\mathbb{Z}_{n}^{2}\times\mathbb{Z}_{n}^{2}\to [0,1],~P\left((x,y),(x',y')\right)=\begin{cases}
  \dfrac{1}{4}同时\pm1, \dfrac{1}{2}同时不变  & ,x=y \\
一个不变, 一个\pm1 & ,x\neq y
\end{cases}$$
则$X,Y$都是马尔科夫链, 但是关心联合分布时: 考虑$X-Y$, 如果为0, 则恒为0; 否则随机加减1.

也即, 以0和$n$为吸收壁的随机游走. 

因此$$\bar{d}(t)\leqslant \Pr[]{X_{t}\neq Y_{t}}=\Pr[]{\tau>t}$$
进而化成容易研究的问题.

